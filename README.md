# abdo_project
<a href="https://github.com/janeabdo">
   <img src="https://avatars.githubusercontent.com/u/160653193?v=4" width="100px;" alt=""/>
   <br /><sub><b>Jane Abdo</b></sub>
</a>

Hey! I'm Jane, a professional master's in biomedical engineering student at Polytechnique Montréal. I'm super excited to start my project! :) 

<h1> Controlling machines with imagination  </h1>
<h3> <strong>Introduction:</strong> </h3>
A variety of movement types can be decoded from brain signals during movement execution, ex: wrist flexion and extension, grabbing, finger moving… (Volkova et al, 2019). These decoded signals can then be used to control external devices, such as a screen cursor, a mouse or a prosthetic limb. Certain handicapped populations, like paralyzed and amputated people, could largely benefit from the control of external devices. As they do not have brain signals associated with the execution of movement, other ways of controlling the external device are needed. Fortunately, studies have shown that motor imagery (imagining executing movement) and motor control (executing movement) share neural mechanisms, by activating similar brain regions (Guillot et al., 2009). 
<br> Hence, the question is: Can we decode movement types based on brain signals from imagined movement? 
<br> There are many ways of extracting brain signals, the least invasive of which (which is actually not invasive at all) is EEG (electro<strong>encephalo</strong>graphy). However, because of the location of the electrodes on the scalp, the signal is distorted by the scalp and skull. Other more invasive techniques include EcoG (electro<strong>cortico</strong>graphy), in which the electrodes are placed on the surface of the cortex. This technique has been shown to yield better signal quality. 
<br> The question becomes: Can we decode movement types based on EcoG brain signals from imagined movement? 
<br> A group of students have laid the foundations for answering this question during an online computational neuroscience course called [Neuromatch](https://compneuro.neuromatch.io/). Amongst other creations, they have developed a classifier to decode movement types from imagined as well as executed movement EcoG signals. This classifier will be the foundation of my project.  
<h3> <strong>Main Objectives:</strong> </h3>
<ul>
<li>Demonstrate better classifier accuracy through improved data processing </li>
<li>Implement other classifiers and compare performances </li>
<li>Apply acquired data visualization concepts on electrophysiological data </li>
</ul>
<h3> <strong>Personal Objectives:</strong> </h3>
<ul>
<li>Develop an understanding of EcoG data (features, filtering, processing…)</li>
<li>Get familiar with Github/Git</li>
<li>Gain an understanding of open science notions<li>
<h3> <strong>Data:</strong> </h3>
The raw data comes from a public source (Miller KJ. A library of human electrocorticographic data and analyses. Nat Hum Behav. 2019 Nov;3(11):1225-1235. doi: 10.1038/s41562-019-0678-3. Epub 2019 Aug 26. PMID: 31451738.). The data used for my project has been downloaded not from the original source (raw data), but from a preprocessed source coming from the Neuromatch Academy website for computational neuroscience (https://osf.io/ksqv8). 
<h3> <strong>Deliverables:</strong> </h3>
Jupyter notebook containing data processing, classifiers and data visualization
Tools:
Methods:
Results:
Conclusion:


